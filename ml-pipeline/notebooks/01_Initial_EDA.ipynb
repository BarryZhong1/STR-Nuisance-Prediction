{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” STR Nuisance Prediction - Multi-City EDA\n",
    "\n",
    "**Objective**: Explore STR and complaint datasets to build nuisance prediction models.\n",
    "\n",
    "**Current City**: Scottsdale (easily configurable for other cities)\n",
    "\n",
    "**System Design**: Multi-city compatible STR analysis framework\n",
    "\n",
    "## ğŸ“Š Analysis Goals:\n",
    "1. ğŸ›ï¸ City-specific data exploration\n",
    "2. ğŸ” Pattern identification across datasets  \n",
    "3. ğŸ  STR-complaint relationship mapping\n",
    "4. ğŸ› ï¸ Feature engineering opportunities\n",
    "5. ğŸ¯ Nuisance prediction target creation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"ğŸ›ï¸ Multi-City STR Nuisance Prediction Analysis\")\n",
    "print(f\"ğŸ“… Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ğŸ“š Libraries loaded successfully!\")\n",
    "print(\"ğŸŒ Framework: Adaptable for any city's STR data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ City Configuration\n",
    "\n",
    "Set up the analysis for your specific city. Currently configured for Scottsdale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ City Configuration\n",
    "CITY_NAME = \"Scottsdale\"  # Change this for other cities\n",
    "FOLDER_ID = \"1FEInC_DsWaQo8XIvydEGL1e0si7wqvFg\"  # Your Google Drive folder\n",
    "\n",
    "print(f\"ğŸ¯ Analysis Target: {CITY_NAME}\")\n",
    "print(f\"ğŸ“ Data Source: Google Drive Folder {FOLDER_ID}\")\n",
    "print(f\"ğŸŒ Framework: Multi-city STR prediction system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ Load City Data\n",
    "sys.path.append('../src')\n",
    "\n",
    "try:\n",
    "    from data_processing.folder_data_loader import STRDataLoader, load_city_str_data\n",
    "    print(\"âœ… Multi-city data loader imported successfully\")\n",
    "    \n",
    "    # Load data for the specified city\n",
    "    print(f\"\\nğŸ”„ Loading {CITY_NAME} STR datasets...\")\n",
    "    print(\"â³ This may take a few minutes for large files...\")\n",
    "    \n",
    "    # Use the general loader function\n",
    "    loader, datasets = load_city_str_data(city_name=CITY_NAME, folder_id=FOLDER_ID)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ {CITY_NAME} data loading completed!\")\n",
    "    \nexcept ImportError as e:\n",
    "    print(f\"âŒ Could not import data loader: {e}\")\n",
    "    print(\"ğŸ“ Make sure you've created: ml-pipeline/src/data_processing/folder_data_loader.py\")\n",
    "    print(\"ğŸ’¡ Use the updated Multi-City compatible version\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    print(\"ğŸ”§ Check your Google Drive folder permissions and file links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Dataset Organization\n",
    "if 'datasets' in locals() and datasets:\n",
    "    # Get datasets organized by category\n",
    "    categorized = loader.get_dataset_by_category()\n",
    "    \n",
    "    print(f\"ğŸ“‹ {CITY_NAME.upper()} DATASET SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Function to show dataset info\n",
    "    def show_dataset_info(df, name, emoji=\"ğŸ“Š\"):\n",
    "        if df is not None:\n",
    "            memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "            print(f\"{emoji} {name}: {df.shape[0]:,} rows Ã— {df.shape[1]} columns ({memory_mb:.1f} MB)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ {name}: Not available\")\n",
    "            return False\n",
    "    \n",
    "    # Extract and display by category\n",
    "    total_loaded = 0\n",
    "    \n",
    "    # Properties\n",
    "    if 'properties' in categorized:\n",
    "        print(\"\\nğŸ  STR PROPERTIES:\")\n",
    "        properties_data = categorized['properties']\n",
    "        \n",
    "        licensed_strs = properties_data.get('licensed_strs')\n",
    "        unlicensed_strs = properties_data.get('unlicensed_strs') \n",
    "        pending_strs = properties_data.get('pending_licences')\n",
    "        \n",
    "        total_loaded += show_dataset_info(licensed_strs, \"Licensed STRs\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(unlicensed_strs, \"Unlicensed STRs\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(pending_strs, \"Pending Applications\", \"âœ…\")\n",
    "    \n",
    "    # Complaints\n",
    "    if 'complaints' in categorized:\n",
    "        print(\"\\nğŸ“ COMPLAINTS & VIOLATIONS:\")\n",
    "        complaints_data = categorized['complaints']\n",
    "        \n",
    "        ez_complaints = complaints_data.get('ez_complaints')\n",
    "        code_violations = complaints_data.get('code_violations')\n",
    "        \n",
    "        total_loaded += show_dataset_info(ez_complaints, \"EZ Complaints System\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(code_violations, \"Code Violations\", \"âœ…\")\n",
    "    \n",
    "    # Police\n",
    "    if 'police' in categorized:\n",
    "        print(\"\\nğŸ‘® POLICE DATA:\")\n",
    "        police_data = categorized['police']\n",
    "        \n",
    "        police_incidents = police_data.get('police_incidents')\n",
    "        police_citations = police_data.get('police_citations')\n",
    "        police_arrests = police_data.get('police_arrests')\n",
    "        \n",
    "        total_loaded += show_dataset_info(police_incidents, \"Incident Reports\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(police_citations, \"Citations Issued\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(police_arrests, \"Arrest Records\", \"âœ…\")\n",
    "    \n",
    "    # Geographic\n",
    "    if 'geographic' in categorized:\n",
    "        print(\"\\nğŸ—ºï¸ GEOGRAPHIC DATA:\")\n",
    "        geographic_data = categorized['geographic']\n",
    "        \n",
    "        parcels = geographic_data.get('parcels')\n",
    "        \n",
    "        total_loaded += show_dataset_info(parcels, \"Property Parcels\", \"âœ…\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ SUMMARY: {total_loaded} datasets successfully loaded\")\n",
    "    print(f\"ğŸ¯ Ready for {CITY_NAME} STR nuisance analysis!\")\n",
    "    \nelse:\n",
    "    print(f\"âŒ No datasets available for {CITY_NAME}\")\n",
    "    print(\"ğŸ”§ Check data loading configuration above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ  STR Properties Deep Dive\n",
    "\n",
    "Analyze the core STR properties dataset for your city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ  Licensed STR Properties Analysis\n",
    "if 'licensed_strs' in locals() and licensed_strs is not None:\n",
    "    print(f\"ğŸ  {CITY_NAME} LICENSED STR PROPERTIES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset Overview:\")\n",
    "    print(f\"   City: {CITY_NAME}\")\n",
    "    print(f\"   Total licensed properties: {len(licensed_strs):,}\")\n",
    "    print(f\"   Columns: {len(licensed_strs.columns)}\")\n",
    "    print(f\"   Memory usage: {licensed_strs.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Column Structure:\")\n",
    "    for i, (col, dtype) in enumerate(zip(licensed_strs.columns, licensed_strs.dtypes), 1):\n",
    "        non_null = licensed_strs[col].count()\n",
    "        null_pct = (len(licensed_strs) - non_null) / len(licensed_strs) * 100\n",
    "        unique_vals = licensed_strs[col].nunique()\n",
    "        \n",
    "        print(f\"   {i:2d}. {col:<35} | {str(dtype):<15} | {null_pct:5.1f}% missing | {unique_vals:,} unique\")\n",
    "    \n",
    "    print(f\"\\nğŸ‘ï¸ Sample {CITY_NAME} Licensed STRs:\")\n",
    "    display(licensed_strs.head())\n",
    "    \n",
    "    # Look for key columns\n",
    "    address_cols = [col for col in licensed_strs.columns if 'address' in col.lower()]\n",
    "    id_cols = [col for col in licensed_strs.columns if 'id' in col.lower()]\n",
    "    date_cols = [col for col in licensed_strs.columns if 'date' in col.lower()]\n",
    "    \n",
    "    print(f\"\\nğŸ” Key Columns Identified:\")\n",
    "    if address_cols:\n",
    "        print(f\"   ğŸ“ Address columns: {address_cols}\")\n",
    "    if id_cols:\n",
    "        print(f\"   ğŸ†” ID columns: {id_cols}\")\n",
    "    if date_cols:\n",
    "        print(f\"   ğŸ“… Date columns: {date_cols}\")\n",
    "    \n",
    "    # Basic statistics for numeric columns\n",
    "    numeric_cols = licensed_strs.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nğŸ“ˆ Numeric Columns Statistics:\")\n",
    "        display(licensed_strs[numeric_cols].describe())\n",
    "        \nelse:\n",
    "    print(f\"âŒ Licensed STR data not available for {CITY_NAME}\")\n",
    "    print(\"ğŸ”§ This is typically the main dataset for STR analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Complaints Data Analysis\n",
    "if 'ez_complaints' in locals() and ez_complaints is not None:\n",
    "    print(f\"ğŸ“ {CITY_NAME} COMPLAINTS ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset Overview:\")\n",
    "    print(f\"   City: {CITY_NAME}\")\n",
    "    print(f\"   Total complaints: {len(ez_complaints):,}\")\n",
    "    print(f\"   Columns: {len(ez_complaints.columns)}\")\n",
    "    print(f\"   Date range: {ez_complaints.shape[0]:,} records\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Complaints Column Structure:\")\n",
    "    for i, col in enumerate(ez_complaints.columns[:10], 1):  # Show first 10 columns\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    if len(ez_complaints.columns) > 10:\n",
    "        print(f\"   ... and {len(ez_complaints.columns) - 10} more columns\")\n",
    "    \n",
    "    print(f\"\\nğŸ‘ï¸ Sample {CITY_NAME} Complaints:\")\n",
    "    display(ez_complaints.head(3))\n",
    "    \n",
    "    # Identify key columns for analysis\n",
    "    complaint_address_cols = [col for col in ez_complaints.columns if 'address' in col.lower()]\n",
    "    complaint_date_cols = [col for col in ez_complaints.columns if 'date' in col.lower()]\n",
    "    complaint_type_cols = [col for col in ez_complaints.columns \n",
    "                          if any(word in col.lower() for word in ['type', 'category', 'subject', 'nature'])]\n",
    "    \n",
    "    print(f\"\\nğŸ” Key Complaint Columns:\")\n",
    "    if complaint_address_cols:\n",
    "        print(f\"   ğŸ“ Address columns: {complaint_address_cols}\")\n",
    "    if complaint_date_cols:\n",
    "        print(f\"   ğŸ“… Date columns: {complaint_date_cols}\")\n",
    "    if complaint_type_cols:\n",
    "        print(f\"   ğŸ“‹ Type columns: {complaint_type_cols}\")\n",
    "\nelse:\n",
    "    print(f\"âŒ Complaints data not available for {CITY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Cross-Dataset Relationship Analysis\n",
    "\n",
    "Explore how to connect STR properties with complaints and other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”— Data Linking Strategy\n",
    "print(f\"ğŸ”— {CITY_NAME} DATA LINKING ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Function to analyze column overlap\n",
    "def analyze_column_overlap(df1, df2, name1, name2):\n",
    "    if df1 is not None and df2 is not None:\n",
    "        cols1 = set(df1.columns)\n",
    "        cols2 = set(df2.columns)\n",
    "        common = cols1.intersection(cols2)\n",
    "        \n",
    "        print(f\"\\nğŸ”— {name1} â†” {name2}:\")\n",
    "        print(f\"   {name1} columns: {len(cols1)}\")\n",
    "        print(f\"   {name2} columns: {len(cols2)}\")\n",
    "        print(f\"   Common columns: {len(common)}\")\n",
    "        \n",
    "        if common:\n",
    "            print(f\"   ğŸ“‹ Shared columns: {sorted(list(common))}\")\n",
    "        \n",
    "        return common\n",
    "    return set()\n",
    "\n",
    "# Analyze key relationships\n",
    "if 'licensed_strs' in locals() and 'ez_complaints' in locals():\n",
    "    if licensed_strs is not None and ez_complaints is not None:\n",
    "        # Main STR-Complaints relationship\n",
    "        str_complaint_common = analyze_column_overlap(\n",
    "            licensed_strs, ez_complaints, \"Licensed STRs\", \"EZ Complaints\"\n",
    "        )\n",
    "\n",
    "# Look for address-based linking opportunities\n",
    "print(f\"\\nğŸ“ ADDRESS-BASED LINKING OPPORTUNITIES:\")\n",
    "\n",
    "datasets_with_addresses = []\n",
    "for name, df in [(\"Licensed STRs\", licensed_strs), (\"EZ Complaints\", ez_complaints), \n",
    "                (\"Police Incidents\", police_incidents if 'police_incidents' in locals() else None),\n",
    "                (\"Parcels\", parcels if 'parcels' in locals() else None)]:\n",
    "    if df is not None:\n",
    "        addr_cols = [col for col in df.columns \n",
    "                    if any(word in col.lower() for word in ['address', 'location', 'street'])]\n",
    "        if addr_cols:\n",
    "            datasets_with_addresses.append((name, addr_cols))\n",
    "            print(f\"   ğŸ“‹ {name}: {addr_cols}\")\n",
    "\n",
    "# Show sample addresses for comparison\n",
    "if len(datasets_with_addresses) >= 2:\n",
    "    print(f\"\\nğŸ” Sample Address Formats for Matching:\")\n",
    "    \n",
    "    for name, addr_cols in datasets_with_addresses[:3]:  # Show first 3 datasets\n",
    "        df = None\n",
    "        if name == \"Licensed STRs\":\n",
    "            df = licensed_strs\n",
    "        elif name == \"EZ Complaints\":\n",
    "            df = ez_complaints\n",
    "        elif name == \"Police Incidents\":\n",
    "            df = police_incidents if 'police_incidents' in locals() else None\n",
    "        elif name == \"Parcels\":\n",
    "            df = parcels if 'parcels' in locals() else None\n",
    "            \n",
    "        if df is not None and addr_cols:\n",
    "            print(f\"\\n   ğŸ“ {name} addresses:\")\n",
    "            sample_addresses = df[addr_cols[0]].dropna().head(3).tolist()\n",
    "            for i, addr in enumerate(sample_addresses, 1):\n",
    "                print(f\"      {i}. {addr}\")\n",
    "\n",
    "print(f\"\\nâœ… Linking analysis completed for {CITY_NAME}\")\n",
    "print(f\"ğŸ¯ Next: Create address standardization and matching strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Temporal Pattern Analysis\n",
    "\n",
    "Analyze time-based patterns in complaints and other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“… Temporal Analysis\n",
    "if 'ez_complaints' in locals() and ez_complaints is not None:\n",
    "    print(f\"ğŸ“… {CITY_NAME} TEMPORAL PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Find date columns\n",
    "    date_columns = [col for col in ez_complaints.columns if 'date' in col.lower()]\n",
    "    \n",
    "    if date_columns:\n",
    "        main_date_col = date_columns[0]  # Use first date column\n",
    "        print(f\"ğŸ“… Analyzing date column: {main_date_col}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert to datetime\n",
    "            ez_complaints[main_date_col] = pd.to_datetime(ez_complaints[main_date_col], errors='coerce')\n",
    "            \n",
    "            # Remove invalid dates\n",
    "            valid_dates = ez_complaints[main_date_col].dropna()\n",
    "            \n",
    "            if len(valid_dates) > 0:\n",
    "                print(f\"\\nğŸ“Š Date Range Analysis:\")\n",
    "                print(f\"   Earliest complaint: {valid_dates.min().date()}\")\n",
    "                print(f\"   Latest complaint: {valid_dates.max().date()}\")\n",
    "                print(f\"   Total span: {(valid_dates.max() - valid_dates.min()).days} days\")\n",
    "                print(f\"   Valid dates: {len(valid_dates):,} ({len(valid_dates)/len(ez_complaints)*100:.1f}%)\")\n",
    "                \n",
    "                # Create temporal features\n",
    "                complaint_dates = ez_complaints[ez_complaints[main_date_col].notna()].copy()\n",
    "                complaint_dates['year'] = complaint_dates[main_date_col].dt.year\n",
    "                complaint_dates['month'] = complaint_dates[main_date_col].dt.month\n",
    "                complaint_dates['day_of_week'] = complaint_dates[main_date_col].dt.dayofweek\n",
    "                \n",
    "                # Yearly trends\n",
    "                if complaint_dates['year'].nunique() > 1:\n",
    "                    print(f\"\\nğŸ“ˆ Yearly Complaint Trends:\")\n",
    "                    yearly_counts = complaint_dates['year'].value_counts().sort_index()\n",
    "                    for year, count in yearly_counts.items():\n",
    "                        print(f\"   {year}: {count:,} complaints\")\n",
    "                    \n",
    "                    # Plot yearly trend\n",
    "                    if len(yearly_counts) > 1:\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        yearly_counts.plot(kind='line', marker='o')\n",
    "                        plt.title(f'{CITY_NAME} Complaints by Year')\n",
    "                        plt.xlabel('Year')\n",
    "                        plt.ylabel('Number of Complaints')\n",
    "                        plt.grid(True, alpha=0.3)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                \n",
    "                # Monthly patterns\n",
    "                print(f\"\\nğŸ“… Seasonal Patterns (Monthly):\")\n",
    "                monthly_counts = complaint_dates['month'].value_counts().sort_index()\n",
    "                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                \n",
    "                for month, count in monthly_counts.items():\n",
    "                    month_name = month_names[month-1]\n",
    "                    print(f\"   {month_name}: {count:,} complaints\")\n",
    "                \n",
    "                # Plot monthly pattern\n",
    "                if len(monthly_counts) > 1:\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    monthly_counts.plot(kind='bar', color='skyblue')\n",
    "                    plt.title(f'{CITY_NAME} Complaints by Month (Seasonal Pattern)')\n",
    "                    plt.xlabel('Month')\n",
    "                    plt.ylabel('Number of Complaints')\n",
    "                    plt.xticks(range(12), month_names, rotation=45)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "            else:\n",
    "                print(\"âŒ No valid dates found in the date column\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing dates: {e}\")\n",
    "            print(f\"Sample values: {ez_complaints[main_date_col].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"âŒ No date columns found in complaints data\")\n",
    "        \nelse:\n",
    "    print(f\"âŒ No complaints data available for temporal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Target Variable Framework\n",
    "\n",
    "Design the framework for creating nuisance prediction targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Target Variable Design Framework\n",
    "print(f\"ğŸ¯ {CITY_NAME} TARGET VARIABLE FRAMEWORK\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"ğŸ“‹ Multi-City Framework Design:\")\n",
    "print(f\"   Current city: {CITY_NAME}\")\n",
    "print(f\"   Adaptable for: Any city with STR and complaint data\")\n",
    "print(f\"   Standardized approach: Yes\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Target Variable Options:\")\n",
    "print(f\"   1. ğŸ“Š Binary Classification:\")\n",
    "print(f\"      - Nuisance (1) vs Not Nuisance (0)\")\n",
    "print(f\"      - Threshold: 2+ complaints per property\")\n",
    "\n",
    "print(f\"\\n   2. ğŸ“ˆ Multi-Class Classification:\")\n",
    "print(f\"      - No Risk (0 complaints)\")\n",
    "print(f\"      - Low Risk (1 complaint)\")\n",
    "print(f\"      - Medium Risk (2-3 complaints)\")\n",
    "print(f\"      - High Risk (4+ complaints)\")\n",
    "\n",
    "print(f\"\\n   3. ğŸ”¢ Regression Target:\")\n",
    "print(f\"      - Complaint count (continuous)\")\n",
    "print(f\"      - Complaint rate (per time period)\")\n",
    "\n",
    "print(f\"\\nğŸ”— Next Steps for {CITY_NAME}:\")\n",
    "print(f\"   1. ğŸ“ Address standardization and matching\")\n",
    "print(f\"   2. ğŸ“Š Link STR properties to complaints\")\n",
    "print(f\"   3. ğŸ“ˆ Calculate complaint frequencies\")\n",
    "print(f\"   4. ğŸ¯ Apply target variable definitions\")\n",
    "print(f\"   5. âœ… Validate target distribution\")\n",
    "\n",
    "print(f\"\\nğŸŒ Multi-City Considerations:\")\n",
    "print(f\"   - Different cities may have different complaint thresholds\")\n",
    "print(f\"   - Address formats vary by city\")\n",
    "print(f\"   - Complaint types and systems differ\")\n",
    "print(f\"   - Framework adapts to local data structure\")\n",
    "\n",
    "print(f\"\\nâœ… Framework design completed for {CITY_NAME}\")\n",
    "print(f\"ğŸš€ Ready to implement city-specific target creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Next Steps & Multi-City Roadmap\n",
    "\n",
    "Summary and next phases for the multi-city STR prediction system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ Analysis Summary and Next Steps\n",
    "print(f\"ğŸ“‹ {CITY_NAME.upper()} ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count successful data loads\n",
    "if 'datasets' in locals():\n",
    "    loaded_count = sum(1 for df in datasets.values() if df is not None)\n",
    "    total_datasets = len(datasets)\n",
    "    \n",
    "    print(f\"ğŸ“Š Data Loading:\")\n",
    "    print(f\"   Successfully loaded: {loaded_count}/{total_datasets} datasets\")\n",
    "    print(f\"   City: {CITY_NAME}\")\n",
    "    print(f\"   Framework: Multi-city compatible\")\n",
    "    \n",
    "    # Calculate total records\n",
    "    total_rows = sum(df.shape[0] for df in datasets.values() if df is not None)\n",
    "    print(f\"   Total records: {total_rows:,}\")\nelse:\n",
    "    print(f\"âŒ No datasets loaded for {CITY_NAME}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ KEY FINDINGS:\")\n",
    "if 'licensed_strs' in locals() and licensed_strs is not None:\n",
    "    print(f\"   âœ… STR Properties: {len(licensed_strs):,} licensed properties\")\nelse:\n",
    "    print(f\"   âŒ STR Properties: Not available\")\n",
    "\n",
    "if 'ez_complaints' in locals() and ez_complaints is not None:\n",
    "    print(f\"   âœ… Complaints: {len(ez_complaints):,} complaint records\")\nelse:\n",
    "    print(f\"   âŒ Complaints: Not available\")\n",
    "\n",
    "print(f\"   âœ… Framework: Ready for multi-city deployment\")\n",
    "print(f\"   âœ… Data structure: Analyzed and documented\")\n",
    "\n",
    "print(f\"\\nğŸš€ IMMEDIATE NEXT STEPS:\")\n",
    "print(f\"   1. ğŸ”— Implement address matching for {CITY_NAME}\")\n",
    "print(f\"   2. ğŸ“Š Create complaint-property linkages\")\n",
    "print(f\"   3. ğŸ¯ Generate target variables\")\n",
    "print(f\"   4. ğŸ› ï¸ Engineer predictive features\")\n",
    "print(f\"   5. ğŸ¤– Train initial ML models\")\n",
    "\n",
    "print(f\"\\nğŸŒ MULTI-CITY EXPANSION ROADMAP:\")\n",
    "print(f\"   Phase 1: Perfect {CITY_NAME} implementation\")\n",
    "print(f\"   Phase 2: Add configuration for 2nd city (Phoenix, Austin, etc.)\")\n",
    "print(f\"   Phase 3: Standardize cross-city features\")\n",
    "print(f\"   Phase 4: Deploy multi-city prediction API\")\n",
    "print(f\"   Phase 5: City-specific dashboard integration\")\n",
    "\n",
    "print(f\"\\nğŸ“… Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ›ï¸ City: {CITY_NAME}\")\n",
    "print(f\"ğŸŒ Framework: Multi-city STR nuisance prediction system\")\n",
    "print(f\"âœ… Status: Ready for feature engineering phase!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Debug Individual File Loading - Test Google Drive Connections\n",
    "from data_processing.folder_data_loader import STRDataLoader\n",
    "import requests\n",
    "\n",
    "print(\"ğŸ” DEBUGGING GOOGLE DRIVE FILE LOADING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize loader for Scottsdale\n",
    "loader = STRDataLoader(city_name=\"Scottsdale\")\n",
    "\n",
    "# Test each file individually\n",
    "test_files = ['licensed_strs', 'ez_complaints', 'unlicensed_strs', 'police_incidents']\n",
    "\n",
    "for file_key in test_files:\n",
    "    config = loader.file_configs[file_key]\n",
    "    print(f\"\\nğŸ” Testing {file_key}:\")\n",
    "    print(f\"   File ID: {config['file_id']}\")\n",
    "    print(f\"   Expected: {config['filename']}\")\n",
    "    print(f\"   Description: {config['description']}\")\n",
    "    \n",
    "    # Test the direct URL first\n",
    "    url = loader.get_direct_download_url(config['file_id'])\n",
    "    print(f\"   ğŸ”— Download URL: {url}\")\n",
    "    \n",
    "    # Test URL accessibility\n",
    "    try:\n",
    "        print(f\"   ğŸ“¡ Testing URL accessibility...\")\n",
    "        response = requests.head(url, timeout=10)\n",
    "        print(f\"   ğŸ“Š Status Code: {response.status_code}\")\n",
    "        print(f\"   ğŸ“ Content Length: {response.headers.get('content-length', 'Unknown')}\")\n",
    "        print(f\"   ğŸ“‹ Content Type: {response.headers.get('content-type', 'Unknown')}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"   âœ… URL is accessible\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Non-200 status code\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ URL test failed: {str(e)}\")\n",
    "    \n",
    "    # Try to load the actual dataset\n",
    "    print(f\"   ğŸ”„ Attempting to load dataset...\")\n",
    "    df = loader.load_dataset(file_key)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(f\"   âœ… SUCCESS: Loaded {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "        print(f\"   ğŸ“‹ Sample columns: {list(df.columns)[:5]}\")\n",
    "        if len(df.columns) > 5:\n",
    "            print(f\"                     ... and {len(df.columns)-5} more\")\n",
    "    else:\n",
    "        print(f\"   âŒ FAILED to load dataset\")\n",
    "        \n",
    "        # Additional debugging for failed loads\n",
    "        try:\n",
    "            print(f\"   ğŸ” Testing raw download...\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            print(f\"   ğŸ“Š Response length: {len(response.text)} characters\")\n",
    "            print(f\"   ğŸ“ First 200 characters: {response.text[:200]}\")\n",
    "        except Exception as debug_e:\n",
    "            print(f\"   âŒ Raw download failed: {str(debug_e)}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Debug Summary:\")\n",
    "print(f\"   ğŸ›ï¸ City: Scottsdale\")\n",
    "print(f\"   ğŸ“ Total configured files: {len(loader.file_configs)}\")\n",
    "print(f\"   ğŸ”§ Framework: Multi-city STR data loader\")\n",
    "\n",
    "print(f\"\\nâœ… Debug analysis completed!\")\n",
    "print(f\"ğŸ’¡ If files show as accessible but loading fails, check Google Drive permissions\")\n",
    "print(f\"ğŸ”— All files should be set to 'Anyone with the link can view'\")"
   ]
}
