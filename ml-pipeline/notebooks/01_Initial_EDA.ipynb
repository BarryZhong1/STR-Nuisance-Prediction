{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” STR Nuisance Prediction - Multi-City EDA\n",
    "\n",
    "**Objective**: Explore STR and complaint datasets to build nuisance prediction models.\n",
    "\n",
    "**Current City**: Scottsdale (easily configurable for other cities)\n",
    "\n",
    "**System Design**: Multi-city compatible STR analysis framework\n",
    "\n",
    "## ğŸ“Š Analysis Goals:\n",
    "1. ğŸ›ï¸ City-specific data exploration\n",
    "2. ğŸ” Pattern identification across datasets  \n",
    "3. ğŸ  STR-complaint relationship mapping\n",
    "4. ğŸ› ï¸ Feature engineering opportunities\n",
    "5. ğŸ¯ Nuisance prediction target creation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"ğŸ›ï¸ Multi-City STR Nuisance Prediction Analysis\")\n",
    "print(f\"ğŸ“… Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ğŸ“š Libraries loaded successfully!\")\n",
    "print(\"ğŸŒ Framework: Adaptable for any city's STR data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ City Configuration\n",
    "\n",
    "Set up the analysis for your specific city. Currently configured for Scottsdale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ City Configuration\n",
    "CITY_NAME = \"Scottsdale\"  # Change this for other cities\n",
    "FOLDER_ID = \"1FEInC_DsWaQo8XIvydEGL1e0si7wqvFg\"  # Your Google Drive folder\n",
    "\n",
    "print(f\"ğŸ¯ Analysis Target: {CITY_NAME}\")\n",
    "print(f\"ğŸ“ Data Source: Google Drive Folder {FOLDER_ID}\")\n",
    "print(f\"ğŸŒ Framework: Multi-city STR prediction system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ Load City Data\n",
    "sys.path.append('../src')\n",
    "\n",
    "try:\n",
    "    from data_processing.folder_data_loader import STRDataLoader, load_city_str_data\n",
    "    print(\"âœ… Multi-city data loader imported successfully\")\n",
    "    \n",
    "    # Load data for the specified city\n",
    "    print(f\"\\nğŸ”„ Loading {CITY_NAME} STR datasets...\")\n",
    "    print(\"â³ This may take a few minutes for large files...\")\n",
    "    \n",
    "    # Use the general loader function\n",
    "    loader, datasets = load_city_str_data(city_name=CITY_NAME, folder_id=FOLDER_ID)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ {CITY_NAME} data loading completed!\")\n",
    "    \nexcept ImportError as e:\n",
    "    print(f\"âŒ Could not import data loader: {e}\")\n",
    "    print(\"ğŸ“ Make sure you've created: ml-pipeline/src/data_processing/folder_data_loader.py\")\n",
    "    print(\"ğŸ’¡ Use the updated Multi-City compatible version\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    print(\"ğŸ”§ Check your Google Drive folder permissions and file links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Dataset Organization\n",
    "if 'datasets' in locals() and datasets:\n",
    "    # Get datasets organized by category\n",
    "    categorized = loader.get_dataset_by_category()\n",
    "    \n",
    "    print(f\"ğŸ“‹ {CITY_NAME.upper()} DATASET SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Function to show dataset info\n",
    "    def show_dataset_info(df, name, emoji=\"ğŸ“Š\"):\n",
    "        if df is not None:\n",
    "            memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "            print(f\"{emoji} {name}: {df.shape[0]:,} rows Ã— {df.shape[1]} columns ({memory_mb:.1f} MB)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ {name}: Not available\")\n",
    "            return False\n",
    "    \n",
    "    # Extract and display by category\n",
    "    total_loaded = 0\n",
    "    \n",
    "    # Properties\n",
    "    if 'properties' in categorized:\n",
    "        print(\"\\nğŸ  STR PROPERTIES:\")\n",
    "        properties_data = categorized['properties']\n",
    "        \n",
    "        licensed_strs = properties_data.get('licensed_strs')\n",
    "        unlicensed_strs = properties_data.get('unlicensed_strs') \n",
    "        pending_strs = properties_data.get('pending_licences')\n",
    "        \n",
    "        total_loaded += show_dataset_info(licensed_strs, \"Licensed STRs\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(unlicensed_strs, \"Unlicensed STRs\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(pending_strs, \"Pending Applications\", \"âœ…\")\n",
    "    \n",
    "    # Complaints\n",
    "    if 'complaints' in categorized:\n",
    "        print(\"\\nğŸ“ COMPLAINTS & VIOLATIONS:\")\n",
    "        complaints_data = categorized['complaints']\n",
    "        \n",
    "        ez_complaints = complaints_data.get('ez_complaints')\n",
    "        code_violations = complaints_data.get('code_violations')\n",
    "        \n",
    "        total_loaded += show_dataset_info(ez_complaints, \"EZ Complaints System\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(code_violations, \"Code Violations\", \"âœ…\")\n",
    "    \n",
    "    # Police\n",
    "    if 'police' in categorized:\n",
    "        print(\"\\nğŸ‘® POLICE DATA:\")\n",
    "        police_data = categorized['police']\n",
    "        \n",
    "        police_incidents = police_data.get('police_incidents')\n",
    "        police_citations = police_data.get('police_citations')\n",
    "        police_arrests = police_data.get('police_arrests')\n",
    "        \n",
    "        total_loaded += show_dataset_info(police_incidents, \"Incident Reports\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(police_citations, \"Citations Issued\", \"âœ…\")\n",
    "        total_loaded += show_dataset_info(police_arrests, \"Arrest Records\", \"âœ…\")\n",
    "    \n",
    "    # Geographic\n",
    "    if 'geographic' in categorized:\n",
    "        print(\"\\nğŸ—ºï¸ GEOGRAPHIC DATA:\")\n",
    "        geographic_data = categorized['geographic']\n",
    "        \n",
    "        parcels = geographic_data.get('parcels')\n",
    "        \n",
    "        total_loaded += show_dataset_info(parcels, \"Property Parcels\", \"âœ…\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ SUMMARY: {total_loaded} datasets successfully loaded\")\n",
    "    print(f\"ğŸ¯ Ready for {CITY_NAME} STR nuisance analysis!\")\n",
    "    \nelse:\n",
    "    print(f\"âŒ No datasets available for {CITY_NAME}\")\n",
    "    print(\"ğŸ”§ Check data loading configuration above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ  STR Properties Deep Dive\n",
    "\n",
    "Analyze the core STR properties dataset for your city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ  Licensed STR Properties Analysis\n",
    "if 'licensed_strs' in locals() and licensed_strs is not None:\n",
    "    print(f\"ğŸ  {CITY_NAME} LICENSED STR PROPERTIES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset Overview:\")\n",
    "    print(f\"   City: {CITY_NAME}\")\n",
    "    print(f\"   Total licensed properties: {len(licensed_strs):,}\")\n",
    "    print(f\"   Columns: {len(licensed_strs.columns)}\")\n",
    "    print(f\"   Memory usage: {licensed_strs.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Column Structure:\")\n",
    "    for i, (col, dtype) in enumerate(zip(licensed_strs.columns, licensed_strs.dtypes), 1):\n",
    "        non_null = licensed_strs[col].count()\n",
    "        null_pct = (len(licensed_strs) - non_null) / len(licensed_strs) * 100\n",
    "        unique_vals = licensed_strs[col].nunique()\n",
    "        \n",
    "        print(f\"   {i:2d}. {col:<35} | {str(dtype):<15} | {null_pct:5.1f}% missing | {unique_vals:,} unique\")\n",
    "    \n",
    "    print(f\"\\nğŸ‘ï¸ Sample {CITY_NAME} Licensed STRs:\")\n",
    "    display(licensed_strs.head())\n",
    "    \n",
    "    # Look for key columns\n",
    "    address_cols = [col for col in licensed_strs.columns if 'address' in col.lower()]\n",
    "    id_cols = [col for col in licensed_strs.columns if 'id'
    }
}
