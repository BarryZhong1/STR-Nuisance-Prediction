{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Target Variable Definition\n",
    "\n",
    "Define what constitutes a \"nuisance\" property for our prediction model based on the analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Define Nuisance Criteria\n",
    "print(\"🎯 DEFINING NUISANCE PROPERTIES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Based on the complaint analysis above, let's define nuisance levels\n",
    "if properties_df is not None and complaints_df is not None:\n",
    "    \n",
    "    # Find merge column again\n",
    "    prop_cols = set(properties_df.columns)\n",
    "    comp_cols = set(complaints_df.columns)\n",
    "    common_cols = prop_cols.intersection(comp_cols)\n",
    "    id_like_cols = [col for col in common_cols \n",
    "                   if any(word in col.lower() for word in ['id', 'property', 'address'])]\n",
    "    \n",
    "    if id_like_cols:\n",
    "        merge_col = id_like_cols[0]\n",
    "        \n",
    "        # Count complaints per property\n",
    "        complaint_counts = complaints_df[merge_col].value_counts()\n",
    "        \n",
    "        # Create a mapping of property to complaint count\n",
    "        property_complaint_map = complaint_counts.to_dict()\n",
    "        \n",
    "        # Add complaint count to properties dataframe\n",
    "        properties_df['complaint_count'] = properties_df[merge_col].map(property_complaint_map).fillna(0)\n",
    "        \n",
    "        # Define nuisance categories\n",
    "        def categorize_nuisance_risk(complaint_count):\n",
    "            \"\"\"Categorize properties based on complaint count\"\"\"\n",
    "            if complaint_count == 0:\n",
    "                return 'No Risk'\n",
    "            elif complaint_count == 1:\n",
    "                return 'Low Risk'\n",
    "            elif complaint_count <= 3:\n",
    "                return 'Medium Risk'\n",
    "            else:\n",
    "                return 'High Risk'\n",
    "        \n",
    "        # Apply categorization\n",
    "        properties_df['nuisance_risk'] = properties_df['complaint_count'].apply(categorize_nuisance_risk)\n",
    "        \n",
    "        # Create binary target for ML\n",
    "        properties_df['is_nuisance'] = (properties_df['complaint_count'] >= 2).astype(int)\n",
    "        \n",
    "        # Analyze the distribution\n",
    "        risk_distribution = properties_df['nuisance_risk'].value_counts()\n",
    "        print(f\"\\n📊 Nuisance Risk Distribution:\")\n",
    "        for risk_level, count in risk_distribution.items():\n",
    "            pct = (count / len(properties_df)) * 100\n",
    "            print(f\"   {risk_level}: {count:,} properties ({pct:.1f}%)\")\n",
    "        \n",
    "        # Binary target distribution\n",
    "        binary_dist = properties_df['is_nuisance'].value_counts()\n",
    "        print(f\"\\n🎯 Binary Target Distribution (2+ complaints = nuisance):\")\n",
    "        print(f\"   Not Nuisance (0): {binary_dist[0]:,} properties ({binary_dist[0]/len(properties_df)*100:.1f}%)\")\n",
    "        print(f\"   Nuisance (1): {binary_dist[1]:,} properties ({binary_dist[1]/len(properties_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Visualize the distributions\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Risk categories\n",
    "        risk_distribution.plot(kind='bar', ax=axes[0], color=['green', 'yellow', 'orange', 'red'])\n",
    "        axes[0].set_title('Nuisance Risk Categories')\n",
    "        axes[0].set_xlabel('Risk Level')\n",
    "        axes[0].set_ylabel('Number of Properties')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Binary target\n",
    "        binary_dist.plot(kind='bar', ax=axes[1], color=['lightblue', 'salmon'])\n",
    "        axes[1].set_title('Binary Target Distribution\\n(Nuisance vs Not Nuisance)')\n",
    "        axes[1].set_xlabel('Classification')\n",
    "        axes[1].set_ylabel('Number of Properties')\n",
    "        axes[1].set_xticklabels(['Not Nuisance', 'Nuisance'], rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n✅ Target variables created:\")\n",
    "        print(f\"   - complaint_count: Number of complaints per property\")\n",
    "        print(f\"   - nuisance_risk: Categorical risk level (No/Low/Medium/High)\")\n",
    "        print(f\"   - is_nuisance: Binary target (1 if 2+ complaints, 0 otherwise)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Cannot create target variables - no common identifier found\")\n",
    "        \nelse:\n",
    "    print(\"❌ Cannot create target variables - missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Feature Engineering Opportunities\n",
    "\n",
    "Identify potential features for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🛠️ FEATURE ENGINEERING OPPORTUNITIES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if properties_df is not None:\n",
    "    print(\"\\n📋 Potential Features from Properties Data:\")\n",
    "    \n",
    "    # Analyze each column for feature potential\n",
    "    for col in properties_df.columns:\n",
    "        if col in ['complaint_count', 'nuisance_risk', 'is_nuisance']:\n",
    "            continue  # Skip target variables\n",
    "        \n",
    "        dtype = properties_df[col].dtype\n",
    "        unique_count = properties_df[col].nunique()\n",
    "        missing_pct = properties_df[col].isnull().mean() * 100\n",
    "        \n",
    "        print(f\"\\n   🔹 {col}:\")\n",
    "        print(f\"      Type: {dtype}\")\n",
    "        print(f\"      Unique values: {unique_count:,}\")\n",
    "        print(f\"      Missing: {missing_pct:.1f}%\")\n",
    "        \n",
    "        # Suggest feature engineering approaches\n",
    "        if 'date' in col.lower() or dtype == 'datetime64[ns]':\n",
    "            print(f\"      💡 Feature ideas: Age calculation, seasonality, day of week\")\n",
    "        elif dtype in ['int64', 'float64'] and unique_count > 10:\n",
    "            print(f\"      💡 Feature ideas: Binning, normalization, polynomial features\")\n",
    "        elif dtype == 'object' and unique_count < 50:\n",
    "            print(f\"      💡 Feature ideas: One-hot encoding, target encoding\")\n",
    "        elif 'address' in col.lower():\n",
    "            print(f\"      💡 Feature ideas: Geocoding, neighborhood clustering\")\n",
    "        elif unique_count == len(properties_df):\n",
    "            print(f\"      💡 Feature ideas: Use as identifier, extract patterns\")\n",
    "\nif complaints_df is not None:\n",
    "    print(\"\\n\\n📋 Potential Features from Complaints Data:\")\n",
    "    print(\"   🔹 Temporal features:\")\n",
    "    print(\"      - Complaints in last 30/90/365 days\")\n",
    "    print(\"      - Seasonal complaint patterns\")\n",
    "    print(\"      - Weekend vs weekday complaints\")\n",
    "    print(\"      - Time since last complaint\")\n",
    "    \n",
    "    print(\"\\n   🔹 Complaint type features:\")\n",
    "    print(\"      - Most common complaint type\")\n",
    "    print(\"      - Complaint type diversity (entropy)\")\n",
    "    print(\"      - Noise complaints ratio\")\n",
    "    \n",
    "    print(\"\\n   🔹 Frequency features:\")\n",
    "    print(\"      - Average complaints per month\")\n",
    "    print(\"      - Complaint trend (increasing/decreasing)\")\n",
    "    print(\"      - Days between complaints (average)\")\n",
    "\nprint(\"\\n\\n🎯 RECOMMENDED FEATURE CATEGORIES:\")\nprint(\"=\" * 40)\nprint(\"\\n1. 📊 Property Characteristics:\")\nprint(\"   - Property type, size, age\")\nprint(\"   - Owner type (individual vs corporate)\")\nprint(\"   - Capacity (bedrooms, occupancy)\")\n\nprint(\"\\n2. 📈 Historical Complaint Patterns:\")\nprint(\"   - Total complaint count\")\nprint(\"   - Recent complaint activity\")\nprint(\"   - Complaint type distribution\")\nprint(\"   - Seasonal patterns\")\n\nprint(\"\\n3. 🌍 Geographic Features:\")\nprint(\"   - Neighborhood characteristics\")\nprint(\"   - Density of nearby STRs\")\nprint(\"   - Distance to entertainment districts\")\n\nprint(\"\\n4. ⏰ Temporal Features:\")\nprint(\"   - Property age (time since permit)\")\nprint(\"   - Seasonality indicators\")\nprint(\"   - Recent activity trends\")\n\nprint(\"\\n5. 🎭 Behavioral Features:\")\nprint(\"   - Response time to complaints\")\nprint(\"   - Repeat complaint patterns\")\nprint(\"   - Violation resolution history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Save Processed Data\n",
    "\n",
    "Save the analyzed data with target variables for the next phase of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Save processed datasets\n",
    "print(\"💾 SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "import os\n",
    "\n",
    "# Create processed data directory\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save datasets with target variables\n",
    "    if properties_df is not None:\n",
    "        properties_file = f'{processed_dir}/properties_with_targets.csv'\n",
    "        properties_df.to_csv(properties_file, index=False)\n",
    "        print(f\"✅ Saved properties with targets: {properties_file}\")\n",
    "        print(f\"   Shape: {properties_df.shape}\")\n",
    "        print(f\"   Columns: {list(properties_df.columns)}\")\n",
    "    \n",
    "    if complaints_df is not None:\n",
    "        complaints_file = f'{processed_dir}/complaints_processed.csv'\n",
    "        complaints_df.to_csv(complaints_file, index=False)\n",
    "        print(f\"\\n✅ Saved processed complaints: {complaints_file}\")\n",
    "        print(f\"   Shape: {complaints_df.shape}\")\n",
    "    \n",
    "    if violations_df is not None:\n",
    "        violations_file = f'{processed_dir}/violations_processed.csv'\n",
    "        violations_df.to_csv(violations_file, index=False)\n",
    "        print(f\"\\n✅ Saved processed violations: {violations_file}\")\n",
    "        print(f\"   Shape: {violations_df.shape}\")\n",
    "    \n",
    "    # Save analysis summary\n",
    "    summary_file = f'{processed_dir}/eda_summary.txt'\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"STR Nuisance Prediction - EDA Summary\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Datasets Analyzed:\\n\")\n",
    "        for name, df in datasets.items():\n",
    "            if df is not None:\n",
    "                f.write(f\"  - {name}: {df.shape[0]:,} rows, {df.shape[1]} columns\\n\")\n",
    "        \n",
    "        if properties_df is not None and 'nuisance_risk' in properties_df.columns:\n",
    "            f.write(\"\\nTarget Variable Distribution:\\n\")\n",
    "            risk_dist = properties_df['nuisance_risk'].value_counts()\n",
    "            for risk, count in risk_dist.items():\n",
    "                pct = count / len(properties_df) * 100\n",
    "                f.write(f\"  - {risk}: {count:,} ({pct:.1f}%)\\n\")\n",
    "    \n",
    "    print(f\"\\n✅ Saved analysis summary: {summary_file}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"❌ Error saving files: {e}\")\n",
    "\n",
    "print(f\"\\n📁 All processed files saved to: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 EDA Summary & Next Steps\n",
    "\n",
    "Summary of findings and recommendations for next phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 EDA ANALYSIS SUMMARY\")\nprint(\"=\" * 30)\n\n# Calculate summary statistics\ntotal_properties = len(properties_df) if properties_df is not None else 0\ntotal_complaints = len(complaints_df) if complaints_df is not None else 0\n\nprint(f\"\\n📊 Dataset Overview:\")\nprint(f\"   Total STR Properties: {total_properties:,}\")\nprint(f\"   Total Complaints: {total_complaints:,}\")\n\nif properties_df is not None and 'complaint_count' in properties_df.columns:\n    properties_with_complaints = (properties_df['complaint_count'] > 0).sum()\n    avg_complaints = properties_df['complaint_count'].mean()\n    max_complaints = properties_df['complaint_count'].max()\n    \n    print(f\"   Properties with complaints: {properties_with_complaints:,} ({properties_with_complaints/total_properties*100:.1f}%)\")\n    print(f\"   Average complaints per property: {avg_complaints:.2f}\")\n    print(f\"   Max complaints for single property: {max_complaints}\")\n\nif properties_df is not None and 'is_nuisance' in properties_df.columns:\n    nuisance_count = properties_df['is_nuisance'].sum()\n    nuisance_rate = nuisance_count / len(properties_df) * 100\n    \n    print(f\"\\n🎯 Target Variable:\")\n    print(f\"   Nuisance properties (2+ complaints): {nuisance_count:,} ({nuisance_rate:.1f}%)\")\n    print(f\"   Class balance: {'Balanced' if 20 <= nuisance_rate <= 80 else 'Imbalanced'}\")\n\nprint(f\"\\n✅ Key Findings:\")\nprint(f\"   1. Target variable successfully created\")\nprint(f\"   2. Class distribution identified\")\nprint(f\"   3. Feature engineering opportunities mapped\")\nprint(f\"   4. Data quality issues documented\")\n\nprint(f\"\\n🎯 Next Steps:\")\nprint(f\"   1. 🔧 Feature Engineering - Create ML-ready features\")\nprint(f\"   2. 🤖 Model Development - Train prediction models\")\nprint(f\"   3. ✅ Model Validation - Evaluate performance\")\nprint(f\"   4. 🚀 Deployment Pipeline - Set up production system\")\n\nprint(f\"\\n📝 Recommended Actions:\")\nif properties_df is not None and complaints_df is not None:\n    if 'is_nuisance' in properties_df.columns:\n        nuisance_rate = properties_df['is_nuisance'].mean() * 100\n        if nuisance_rate < 10:\n            print(f\"   ⚠️  Low nuisance rate ({nuisance_rate:.1f}%) - consider adjusting threshold\")\n        elif nuisance_rate > 50:\n            print(f\"   ⚠️  High nuisance rate ({nuisance_rate:.1f}%) - consider stricter threshold\")\n        else:\n            print(f\"   ✅ Good target balance ({nuisance_rate:.1f}%) for ML modeling\")\n\nprint(f\"\\n📅 Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"🎉 Ready to proceed to feature engineering phase!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
    "# 🔍 STR Nuisance Prediction - Initial EDA\n",
    "\n",
    "**Objective**: Explore STR and complaint datasets to understand patterns and identify features for nuisance prediction.\n",
    "\n",
    "**Dataset Source**: Google Drive folder with Scottsdale data\n",
    "\n",
    "## Analysis Goals:\n",
    "1. 📊 Understanding data structure and quality\n",
    "2. 🔍 Identifying nuisance patterns and trends  \n",
    "3. 🏠 Exploring relationships between properties and complaints\n",
    "4. 🛠️ Feature engineering opportunities\n",
    "5. 🎯 Target variable definition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"📚 Libraries imported successfully!\")\n",
    "print(f\"🗓️ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"🚀 Ready to analyze STR data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Data Loading from Google Drive\n",
    "\n",
    "Loading datasets from your Google Drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project path to Python path\n",
    "project_root = os.path.abspath('../src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import our custom data loader\n",
    "try:\n",
    "    from data_processing.folder_data_loader import STRDataLoader\n",
    "    print(\"✅ Custom data loader imported successfully\")\nexcept ImportError as e:\n",
    "    print(f\"❌ Error importing data loader: {e}\")\n",
    "    print(\"📝 Please create the folder_data_loader.py file first\")\n",
    "    # Fallback: we'll create sample data\n",
    "    STRDataLoader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Initialize Data Loader\n",
    "if STRDataLoader is not None:\n",
    "    # Your Google Drive folder ID\n",
    "    folder_id = \"1FEInC_DsWaQo8XIvydEGL1e0si7wqvFg\"\n",
    "    loader = STRDataLoader(folder_id=folder_id)\n",
    "    \n",
    "    print(\"🔗 Google Drive folder connected!\")\n",
    "    print(f\"📁 Folder ID: {folder_id}\")\n",
    "    \n",
    "    # Show instructions for getting individual file links\n",
    "    loader.get_folder_contents_instruction()\n",
    "    \nelse:\n",
    "    print(\"⚠️  Data loader not available, using sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Load Your Actual Data\n",
    "\n",
    "**IMPORTANT**: To load your real data, please:\n",
    "1. Go to your Google Drive folder\n",
    "2. Get individual sharing links for each CSV file\n",
    "3. Update the `file_links` dictionary below\n",
    "4. Re-run this cell\n",
    "\n",
    "For now, we'll load sample data to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 Load Datasets\n",
    "if STRDataLoader is not None:\n",
    "    \n",
    "    # OPTION 1: Load with individual file links (RECOMMENDED)\n",
    "    # Uncomment and update these with your actual Google Drive file sharing links:\n",
    "    \n",
    "    \"\"\"\n",
    "    file_links = {\n",
    "        'complaints': 'https://drive.google.com/file/d/YOUR_COMPLAINTS_FILE_ID/view?usp=sharing',\n",
    "        'properties': 'https://drive.google.com/file/d/YOUR_PROPERTIES_FILE_ID/view?usp=sharing',\n",
    "        'violations': 'https://drive.google.com/file/d/YOUR_VIOLATIONS_FILE_ID/view?usp=sharing',\n",
    "    }\n",
    "    \n",
    "    print(\"🔄 Loading your actual datasets from Google Drive...\")\n",
    "    loader.setup_with_file_urls(file_links)\n",
    "    datasets = loader.get_all_datasets()\n",
    "    \"\"\"\n",
    "    \n",
    "    # OPTION 2: For now, load sample data for testing\n",
    "    print(\"🔄 Loading sample data for development and testing...\")\n",
    "    datasets = loader.get_all_datasets()\n",
    "    \n",
    "    # Print summary of loaded data\n",
    "    loader.print_summary()\n",
    "    \nelse:\n",
    "    # Create minimal sample data if loader not available\n",
    "    print(\"🔧 Creating basic sample data...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_properties = 500\n",
    "    \n",
    "    datasets = {\n",
    "        'properties': pd.DataFrame({\n",
    "            'property_id': range(1, n_properties + 1),\n",
    "            'address': [f\"{i} Sample Street\" for i in range(1, n_properties + 1)],\n",
    "            'property_type': np.random.choice(['House', 'Condo', 'Townhouse'], n_properties),\n",
    "            'bedrooms': np.random.choice([1, 2, 3, 4], n_properties)\n",
    "        }),\n",
    "        'complaints': pd.DataFrame({\n",
    "            'complaint_id': range(1, 1001),\n",
    "            'property_id': np.random.choice(range(1, n_properties + 1), 1000),\n",
    "            'complaint_type': np.random.choice(['Noise', 'Parking', 'Trash'], 1000),\n",
    "            'complaint_date': pd.date_range('2022-01-01', periods=1000, freq='D')\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Sample data created for testing\")\n",
    "\n",
    "# Extract individual datasets for analysis\n",
    "properties_df = datasets.get('properties')\n",
    "complaints_df = datasets.get('complaints')\n",
    "violations_df = datasets.get('violations')\n",
    "\n",
    "print(f\"\\n📊 Available datasets: {list(datasets.keys())}\")\n",
    "print(\"✅ Data loading completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Dataset Structure Analysis\n",
    "\n",
    "Let's examine the structure and quality of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df, dataset_name):\n",
    "    \"\"\"Comprehensive analysis of a dataset\"\"\"\n",
    "    if df is None:\n",
    "        print(f\"❌ {dataset_name} dataset not available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📋 {dataset_name.upper()} DATASET ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic information\n",
    "    rows, cols = df.shape\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    \n",
    "    print(f\"📏 Shape: {rows:,} rows × {cols} columns\")\n",
    "    print(f\"💾 Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "    # Column information\n",
    "    print(f\"\\n📊 Column Details:\")\n",
    "    for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):\n",
    "        non_null = df[col].count()\n",
    "        null_pct = ((rows - non_null) / rows * 100)\n",
    "        unique_vals = df[col].nunique()\n",
    "        \n",
    "        print(f\"   {i:2d}. {col:25} | {str(dtype):10} | {non_null:6,} non-null | {null_pct:5.1f}% missing | {unique_vals:6,} unique\")\n",
    "    \n",
    "    # Missing values summary\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n❌ Missing Values Summary:\")\n",
    "        missing_pct = (missing / len(df)) * 100\n",
    "        missing_summary = pd.DataFrame({\n",
    "            'Count': missing[missing > 0],\n",
    "            'Percentage': missing_pct[missing > 0].round(1)\n",
    "        }).sort_values('Percentage', ascending=False)\n",
    "        print(missing_summary.to_string())\n",
    "    else:\n",
    "        print(f\"\\n✅ No missing values!\")\n",
    "    \n",
    "    # Data types summary\n",
    "    print(f\"\\n📋 Data Types Summary:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(f\"\\n👁️  First 3 Rows:\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze each dataset\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        analyze_dataset(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏠 Properties Dataset Deep Dive\n",
    "\n",
    "Detailed analysis of the STR properties dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if properties_df is not None:\n",
    "    print(\"🏠 STR PROPERTIES ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Numeric columns analysis\n",
    "    numeric_cols = properties_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(f\"\\n📈 Numeric Columns Statistics:\")\n",
    "        display(properties_df[numeric_cols].describe())\n",
    "        \n",
    "        # Create visualizations for numeric columns\n",
    "        n_numeric = len(numeric_cols)\n",
    "        if n_numeric > 0:\n",
    "            fig, axes = plt.subplots(nrows=(n_numeric + 1) // 2, ncols=2, figsize=(15, 4 * ((n_numeric + 1) // 2)))\n",
    "            if n_numeric == 1:\n",
    "                axes = [axes]\n",
    "            elif n_numeric > 2:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for i, col in enumerate(numeric_cols[:4]):  # Limit to first 4 numeric columns\n",
    "                ax = axes[i] if n_numeric > 1 else axes\n",
    "                properties_df[col].hist(bins=30, ax=ax, alpha=0.7)\n",
    "                ax.set_title(f'Distribution of {col}')\n",
    "                ax.set_xlabel(col)\n",
    "                ax.set_ylabel('Frequency')\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for i in range(len(numeric_cols), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = properties_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"\\n📊 Categorical Columns Analysis:\")\n",
    "        \n",
    "        for col in categorical_cols[:3]:  # Analyze first 3 categorical columns\n",
    "            unique_count = properties_df[col].nunique()\n",
    "            print(f\"\\n   📋 {col}:\")\n",
    "            print(f\"      Unique values: {unique_count}\")\n",
    "            \n",
    "            if unique_count <= 20:  # Only show value counts for columns with few unique values\n",
    "                value_counts = properties_df[col].value_counts().head(10)\n",
    "                print(f\"      Top values:\")\n",
    "                for val, count in value_counts.items():\n",
    "                    pct = (count / len(properties_df)) * 100\n",
    "                    print(f\"        {val}: {count:,} ({pct:.1f}%)\")\n",
    "                \n",
    "                # Create visualization if reasonable number of categories\n",
    "                if unique_count <= 10:\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    value_counts.plot(kind='bar')\n",
    "                    plt.title(f'Distribution of {col}')\n",
    "                    plt.xlabel(col)\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            else:\n",
    "                print(f\"      (Too many unique values to display - {unique_count})\")\n",
    "\nelse:\n",
    "    print(\"❌ Properties dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📞 Complaints Dataset Deep Dive\n",
    "\n",
    "Detailed analysis of the complaints dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if complaints_df is not None:\n",
    "    print(\"📞 COMPLAINTS ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Look for date columns\n",
    "    date_like_cols = [col for col in complaints_df.columns \n",
    "                     if any(word in col.lower() for word in ['date', 'time', 'created', 'filed', 'received'])]\n",
    "    \n",
    "    if date_like_cols:\n",
    "        print(f\"\\n📅 Date Columns Found: {date_like_cols}\")\n",
    "        \n",
    "        # Analyze first date column\n",
    "        main_date_col = date_like_cols[0]\n",
    "        print(f\"\\n   Analyzing: {main_date_col}\")\n",
    "        \n",
    "        # Try to convert to datetime\n",
    "        try:\n",
    "            complaints_df[main_date_col] = pd.to_datetime(complaints_df[main_date_col])\n",
    "            \n",
    "            # Date range analysis\n",
    "            min_date = complaints_df[main_date_col].min()\n",
    "            max_date = complaints_df[main_date_col].max()\n",
    "            date_range = max_date - min_date\n",
    "            \n",
    "            print(f\"   📊 Date Range: {min_date.date()} to {max_date.date()}\")\n",
    "            print(f\"   📏 Total span: {date_range.days} days ({date_range.days/365:.1f} years)\")\n",
    "            \n",
    "            # Temporal analysis\n",
    "            complaints_df['year'] = complaints_df[main_date_col].dt.year\n",
    "            complaints_df['month'] = complaints_df[main_date_col].dt.month\n",
    "            complaints_df['day_of_week'] = complaints_df[main_date_col].dt.dayofweek\n",
    "            \n",
    "            # Yearly trends\n",
    "            if complaints_df['year'].nunique() > 1:\n",
    "                yearly_counts = complaints_df['year'].value_counts().sort_index()\n",
    "                print(f\"\\n   📈 Complaints by Year:\")\n",
    "                for year, count in yearly_counts.items():\n",
    "                    print(f\"      {year}: {count:,} complaints\")\n",
    "                \n",
    "                # Plot yearly trend\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                yearly_counts.plot(kind='bar')\n",
    "                plt.title('Complaints by Year')\n",
    "                plt.xlabel('Year')\n",
    "                plt.ylabel('Number of Complaints')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Monthly patterns\n",
    "            monthly_counts = complaints_df['month'].value_counts().sort_index()\n",
    "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            \n",
    "            print(f\"\\n   📅 Seasonal Patterns:\")\n",
    "            for month, count in monthly_counts.items():\n",
    "                month_name = month_names[month-1]\n",
    "                print(f\"      {month_name}: {count:,} complaints\")\n",
    "            \n",
    "            # Plot monthly pattern\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            monthly_counts.plot(kind='bar')\n",
    "            plt.title('Complaints by Month (Seasonal Pattern)')\n",
    "            plt.xlabel('Month')\n",
    "            plt.ylabel('Number of Complaints')\n",
    "            plt.xticks(range(12), month_names, rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error processing date column: {e}\")\n",
    "            print(f\"   Sample values: {complaints_df[main_date_col].head().tolist()}\")\n",
    "    \n",
    "    # Look for complaint type columns\n",
    "    type_like_cols = [col for col in complaints_df.columns \n",
    "                     if any(word in col.lower() for word in ['type', 'category', 'nature', 'reason', 'violation'])]\n",
    "    \n",
    "    if type_like_cols:\n",
    "        print(f\"\\n📋 Complaint Type Columns: {type_like_cols}\")\n",
    "        \n",
    "        # Analyze first type column\n",
    "        main_type_col = type_like_cols[0]\n",
    "        print(f\"\\n   Analyzing: {main_type_col}\")\n",
    "        \n",
    "        type_counts = complaints_df[main_type_col].value_counts().head(10)\n",
    "        print(f\"   📊 Top Complaint Types:\")\n",
    "        \n",
    "        for complaint_type, count in type_counts.items():\n",
    "            pct = (count / len(complaints_df)) * 100\n",
    "            print(f\"      {complaint_type}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Plot complaint types\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        type_counts.plot(kind='barh')\n",
    "        plt.title(f'Top Complaint Types: {main_type_col}')\n",
    "        plt.xlabel('Number of Complaints')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\nelse:\n",
    "    print(\"❌ Complaints dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Property-Complaint Relationship Analysis\n",
    "\n",
    "Analyze the relationship between properties and complaints to understand nuisance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if properties_df is not None and complaints_df is not None:\n",
    "    print(\"🔗 PROPERTY-COMPLAINT RELATIONSHIP ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find common identifier columns\n",
    "    prop_cols = set(properties_df.columns)\n",
    "    comp_cols = set(complaints_df.columns)\n",
    "    common_cols = prop_cols.intersection(comp_cols)\n",
    "    \n",
    "    print(f\"📊 Common columns between datasets: {list(common_cols)}\")\n",
    "    \n",
    "    # Look for property ID columns\n",
    "    id_like_cols = [col for col in common_cols \n",
    "                   if any(word in col.lower() for word in ['id', 'property', 'address'])]\n",
    "    \n",
    "    if id_like_cols:\n",
    "        # Try to merge on first common ID column\n",
    "        merge_col = id_like_cols[0]\n",
    "        print(f\"\\n🔗 Attempting to merge on: {merge_col}\")\n",
    "        \n",
    "        try:\n",
    "            # Complaint counts per property\n",
    "            complaint_counts = complaints_df[merge_col].value_counts()\n",
    "            \n",
    "            print(f\"\\n📈 Complaint Distribution:\")\n",
    "            print(f\"   Total unique properties with complaints: {len(complaint_counts):,}\")\n",
    "            print(f\"   Total properties in dataset: {len(properties_df):,}\")\n",
    "            print(f\"   Properties without complaints: {len(properties_df) - len(complaint_counts):,}\")\n",
    "            \n",
    "            # Analyze complaint frequency distribution\n",
    "            freq_dist = complaint_counts.value_counts().sort_index()\n",
    "            print(f\"\\n📊 Complaint Frequency Distribution:\")\n",
    "            for num_complaints, num_properties in freq_dist.head(10).items():\n",
    "                pct = (num_properties / len(complaint_counts)) * 100\n",
    "                print(f\"   {num_complaints} complaints: {num_properties:,} properties ({pct:.1f}%)\")\n",
    "            \n",
    "            # Plot complaint distribution\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Subplot 1: Complaint frequency distribution\n",
    "            plt.subplot(1, 2, 1)\n",
    "            freq_dist.head(20).plot(kind='bar')\n",
    "            plt.title('Distribution of Complaint Frequency\\n(How many properties have X complaints)')\n",
    "            plt.xlabel('Number of Complaints')\n",
    "            plt.ylabel('Number of Properties')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            # Subplot 2: Top properties by complaint count\n",
    "            plt.subplot(1, 2, 2)\n",
    "            complaint_counts.head(20).plot(kind='bar')\n",
    "            plt.title('Top 20 Properties by Complaint Count')\n",
    "            plt.xlabel('Property ID')\n",
    "            plt.ylabel('Number of Complaints')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Define potential nuisance thresholds\n",
    "            print(f\"\\n🎯 Potential Nuisance Thresholds:\")\n",
    "            for threshold in [1, 2, 3, 5, 10]:\n",
    "                high_complaint_props = (complaint_counts >= threshold).sum()\n",
    "                pct_of_total = (high_complaint_props / len(properties_df)) * 100\n",
    "                pct_of_complained = (high_complaint_props / len(complaint_counts)) * 100\n",
    "                print(f\"   {threshold}+ complaints: {high_complaint_props:,} properties ({pct_of_total:.1f}% of all, {pct_of_complained:.1f}% of those with complaints)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in merge analysis: {e}\")\n",
    "            print(f\"Sample values from {merge_col}:\")\n",
    "            print(f\"Properties: {properties_df[merge_col].head().tolist()}\")\n",
    "            print(f\"Complaints: {complaints_df[merge_col].head().tolist()}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No common identifier columns found for merging\")\n",
    "        print(f\"Properties columns: {list(properties_df.columns)}\")\n",
    "        print(f\"Complaints columns: {list(complaints_df.columns)}\")\n",
    "        \nelse:\n",
    "    print(\"❌ Cannot perform relationship analysis - missing properties or complaints data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
